{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KuRPet522PAc"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QvQ8ok6A2PAk"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.13, 0.31)])\n",
        "\n",
        "# train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
        "# test_dataset = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "uuF5N74h2PAn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Инициализация весов\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')  # He Initialization\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "# Генератор\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.fc4 = nn.Linear(1024, g_output_dim)\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.bn1(self.fc1(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn2(self.fc2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn3(self.fc3(x)), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "# Дискриминатор\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        return torch.sigmoid(self.fc4(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-vNQl6ei2PAp"
      },
      "outputs": [],
      "source": [
        "# build network\n",
        "Z_DIM = 50\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = Z_DIM, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF9ID_zZ2PAq",
        "outputId": "f6502349-2a48-4485-ab5d-3cdfbb90694b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (fc1): Linear(in_features=50, out_features=512, bias=True)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc4): Linear(in_features=1024, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWuqDVHY2PAr",
        "outputId": "834ae2d4-0a30-4ed0-ed47-fc670a1ab8d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "T5vSAMA82PAt"
      },
      "outputs": [],
      "source": [
        "# loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# optimizer\n",
        "G_lr = 0.0001\n",
        "D_lr = 0.00002\n",
        "G_optimizer = optim.Adam(G.parameters(), lr = G_lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = D_lr, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "1h9ChGeQ2PAu"
      },
      "outputs": [],
      "source": [
        "def D_train(x):\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(BATCH_SIZE, 1)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_real)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "    D_real_score = D_output\n",
        "\n",
        "    # train discriminator on fake\n",
        "    z = Variable(torch.randn(BATCH_SIZE, Z_DIM).to(device))\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(BATCH_SIZE, 1).to(device))\n",
        "\n",
        "    D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)\n",
        "    D_fake_score = D_output\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = D_real_loss + D_fake_loss\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    return  D_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "SP1auDNV2PAw"
      },
      "outputs": [],
      "source": [
        "def G_train(x):\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = Variable(torch.randn(BATCH_SIZE, Z_DIM).to(device))\n",
        "    y = Variable(torch.ones(BATCH_SIZE, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    D_output = D(G_output)\n",
        "    G_loss = criterion(D_output, y)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_loss.data.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_z = Variable(torch.randn(BATCH_SIZE, Z_DIM).to(device))\n",
        "def generate_test_image(epoch):\n",
        "    with torch.no_grad():\n",
        "        generated = G(test_z)\n",
        "        save_image(generated.view(generated.size(0), 1, 28, 28)[0],\n",
        "                   f'./output/sample_{epoch}.png')"
      ],
      "metadata": {
        "id": "8aCNezrJMOME"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hUDVNSB2PAx",
        "outputId": "38449c52-7824-48eb-fbcb-d2c9117c785c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/300]: loss_d: 1.916, loss_g: 0.642\n",
            "[2/300]: loss_d: 1.553, loss_g: 0.658\n",
            "[3/300]: loss_d: 1.422, loss_g: 0.631\n",
            "[4/300]: loss_d: 1.344, loss_g: 0.642\n",
            "[5/300]: loss_d: 1.288, loss_g: 0.663\n",
            "[6/300]: loss_d: 1.258, loss_g: 0.673\n",
            "[7/300]: loss_d: 1.213, loss_g: 0.700\n",
            "[8/300]: loss_d: 1.161, loss_g: 0.740\n",
            "[9/300]: loss_d: 1.106, loss_g: 0.794\n",
            "[10/300]: loss_d: 1.055, loss_g: 0.852\n",
            "[11/300]: loss_d: 1.007, loss_g: 0.919\n",
            "[12/300]: loss_d: 0.971, loss_g: 0.978\n",
            "[13/300]: loss_d: 0.924, loss_g: 1.052\n",
            "[14/300]: loss_d: 0.877, loss_g: 1.131\n",
            "[15/300]: loss_d: 0.833, loss_g: 1.219\n",
            "[16/300]: loss_d: 0.803, loss_g: 1.290\n",
            "[17/300]: loss_d: 0.767, loss_g: 1.357\n",
            "[18/300]: loss_d: 0.735, loss_g: 1.432\n",
            "[19/300]: loss_d: 0.705, loss_g: 1.497\n",
            "[20/300]: loss_d: 0.673, loss_g: 1.568\n",
            "[21/300]: loss_d: 0.659, loss_g: 1.631\n",
            "[22/300]: loss_d: 0.634, loss_g: 1.676\n",
            "[23/300]: loss_d: 0.617, loss_g: 1.727\n",
            "[24/300]: loss_d: 0.602, loss_g: 1.770\n",
            "[25/300]: loss_d: 0.586, loss_g: 1.821\n",
            "[26/300]: loss_d: 0.574, loss_g: 1.855\n",
            "[27/300]: loss_d: 0.562, loss_g: 1.880\n",
            "[28/300]: loss_d: 0.557, loss_g: 1.897\n",
            "[29/300]: loss_d: 0.543, loss_g: 1.945\n",
            "[30/300]: loss_d: 0.538, loss_g: 1.955\n",
            "[31/300]: loss_d: 0.523, loss_g: 2.008\n",
            "[32/300]: loss_d: 0.523, loss_g: 1.986\n",
            "[33/300]: loss_d: 0.505, loss_g: 2.046\n",
            "[34/300]: loss_d: 0.497, loss_g: 2.081\n",
            "[35/300]: loss_d: 0.490, loss_g: 2.114\n",
            "[36/300]: loss_d: 0.497, loss_g: 2.096\n",
            "[37/300]: loss_d: 0.483, loss_g: 2.133\n",
            "[38/300]: loss_d: 0.474, loss_g: 2.154\n",
            "[39/300]: loss_d: 0.467, loss_g: 2.179\n",
            "[40/300]: loss_d: 0.457, loss_g: 2.219\n",
            "[41/300]: loss_d: 0.464, loss_g: 2.202\n",
            "[42/300]: loss_d: 0.449, loss_g: 2.267\n",
            "[43/300]: loss_d: 0.442, loss_g: 2.272\n",
            "[44/300]: loss_d: 0.446, loss_g: 2.263\n",
            "[45/300]: loss_d: 0.441, loss_g: 2.271\n",
            "[46/300]: loss_d: 0.432, loss_g: 2.295\n",
            "[47/300]: loss_d: 0.430, loss_g: 2.321\n",
            "[48/300]: loss_d: 0.427, loss_g: 2.332\n",
            "[49/300]: loss_d: 0.417, loss_g: 2.361\n",
            "[50/300]: loss_d: 0.410, loss_g: 2.376\n",
            "[51/300]: loss_d: 0.414, loss_g: 2.380\n",
            "[52/300]: loss_d: 0.409, loss_g: 2.393\n",
            "[53/300]: loss_d: 0.405, loss_g: 2.404\n",
            "[54/300]: loss_d: 0.401, loss_g: 2.427\n",
            "[55/300]: loss_d: 0.396, loss_g: 2.450\n",
            "[56/300]: loss_d: 0.396, loss_g: 2.436\n",
            "[57/300]: loss_d: 0.397, loss_g: 2.439\n",
            "[58/300]: loss_d: 0.391, loss_g: 2.454\n",
            "[59/300]: loss_d: 0.383, loss_g: 2.478\n",
            "[60/300]: loss_d: 0.378, loss_g: 2.519\n",
            "[61/300]: loss_d: 0.379, loss_g: 2.508\n",
            "[62/300]: loss_d: 0.375, loss_g: 2.510\n",
            "[63/300]: loss_d: 0.372, loss_g: 2.517\n",
            "[64/300]: loss_d: 0.367, loss_g: 2.569\n",
            "[65/300]: loss_d: 0.369, loss_g: 2.533\n",
            "[66/300]: loss_d: 0.367, loss_g: 2.575\n",
            "[67/300]: loss_d: 0.360, loss_g: 2.567\n",
            "[68/300]: loss_d: 0.355, loss_g: 2.572\n",
            "[69/300]: loss_d: 0.353, loss_g: 2.616\n",
            "[70/300]: loss_d: 0.355, loss_g: 2.608\n",
            "[71/300]: loss_d: 0.351, loss_g: 2.607\n",
            "[72/300]: loss_d: 0.350, loss_g: 2.635\n",
            "[73/300]: loss_d: 0.348, loss_g: 2.631\n",
            "[74/300]: loss_d: 0.343, loss_g: 2.645\n",
            "[75/300]: loss_d: 0.340, loss_g: 2.663\n",
            "[76/300]: loss_d: 0.340, loss_g: 2.660\n",
            "[77/300]: loss_d: 0.340, loss_g: 2.663\n",
            "[78/300]: loss_d: 0.338, loss_g: 2.665\n",
            "[79/300]: loss_d: 0.332, loss_g: 2.673\n",
            "[80/300]: loss_d: 0.331, loss_g: 2.691\n",
            "[81/300]: loss_d: 0.327, loss_g: 2.699\n",
            "[82/300]: loss_d: 0.325, loss_g: 2.717\n",
            "[83/300]: loss_d: 0.320, loss_g: 2.761\n",
            "[84/300]: loss_d: 0.326, loss_g: 2.710\n",
            "[85/300]: loss_d: 0.320, loss_g: 2.751\n",
            "[86/300]: loss_d: 0.316, loss_g: 2.762\n",
            "[87/300]: loss_d: 0.312, loss_g: 2.783\n",
            "[88/300]: loss_d: 0.317, loss_g: 2.738\n",
            "[89/300]: loss_d: 0.312, loss_g: 2.775\n",
            "[90/300]: loss_d: 0.311, loss_g: 2.778\n",
            "[91/300]: loss_d: 0.310, loss_g: 2.787\n",
            "[92/300]: loss_d: 0.305, loss_g: 2.804\n",
            "[93/300]: loss_d: 0.304, loss_g: 2.816\n",
            "[94/300]: loss_d: 0.304, loss_g: 2.799\n",
            "[95/300]: loss_d: 0.289, loss_g: 2.901\n",
            "[96/300]: loss_d: 0.300, loss_g: 2.832\n",
            "[97/300]: loss_d: 0.297, loss_g: 2.845\n",
            "[98/300]: loss_d: 0.294, loss_g: 2.864\n",
            "[99/300]: loss_d: 0.294, loss_g: 2.855\n",
            "[100/300]: loss_d: 0.291, loss_g: 2.875\n",
            "[101/300]: loss_d: 0.292, loss_g: 2.867\n",
            "[102/300]: loss_d: 0.285, loss_g: 2.911\n",
            "[103/300]: loss_d: 0.285, loss_g: 2.898\n",
            "[104/300]: loss_d: 0.284, loss_g: 2.889\n",
            "[105/300]: loss_d: 0.285, loss_g: 2.900\n",
            "[106/300]: loss_d: 0.276, loss_g: 2.937\n",
            "[107/300]: loss_d: 0.278, loss_g: 2.962\n",
            "[108/300]: loss_d: 0.276, loss_g: 2.941\n",
            "[109/300]: loss_d: 0.279, loss_g: 2.925\n",
            "[110/300]: loss_d: 0.267, loss_g: 2.997\n",
            "[111/300]: loss_d: 0.273, loss_g: 2.968\n",
            "[112/300]: loss_d: 0.269, loss_g: 2.977\n",
            "[113/300]: loss_d: 0.267, loss_g: 3.005\n",
            "[114/300]: loss_d: 0.273, loss_g: 2.972\n",
            "[115/300]: loss_d: 0.267, loss_g: 2.990\n",
            "[116/300]: loss_d: 0.267, loss_g: 2.979\n",
            "[117/300]: loss_d: 0.264, loss_g: 3.019\n",
            "[118/300]: loss_d: 0.261, loss_g: 3.033\n",
            "[119/300]: loss_d: 0.261, loss_g: 3.035\n",
            "[120/300]: loss_d: 0.263, loss_g: 3.014\n",
            "[121/300]: loss_d: 0.258, loss_g: 3.052\n",
            "[122/300]: loss_d: 0.258, loss_g: 3.036\n",
            "[123/300]: loss_d: 0.258, loss_g: 3.058\n",
            "[124/300]: loss_d: 0.254, loss_g: 3.063\n",
            "[125/300]: loss_d: 0.254, loss_g: 3.080\n",
            "[126/300]: loss_d: 0.251, loss_g: 3.068\n",
            "[127/300]: loss_d: 0.256, loss_g: 3.054\n",
            "[128/300]: loss_d: 0.249, loss_g: 3.100\n",
            "[129/300]: loss_d: 0.251, loss_g: 3.112\n",
            "[130/300]: loss_d: 0.248, loss_g: 3.111\n",
            "[131/300]: loss_d: 0.248, loss_g: 3.128\n",
            "[132/300]: loss_d: 0.243, loss_g: 3.152\n",
            "[133/300]: loss_d: 0.246, loss_g: 3.134\n",
            "[134/300]: loss_d: 0.239, loss_g: 3.172\n",
            "[135/300]: loss_d: 0.239, loss_g: 3.173\n",
            "[136/300]: loss_d: 0.249, loss_g: 3.130\n",
            "[137/300]: loss_d: 0.238, loss_g: 3.190\n",
            "[138/300]: loss_d: 0.239, loss_g: 3.164\n",
            "[139/300]: loss_d: 0.237, loss_g: 3.191\n",
            "[140/300]: loss_d: 0.241, loss_g: 3.160\n",
            "[141/300]: loss_d: 0.243, loss_g: 3.148\n",
            "[142/300]: loss_d: 0.235, loss_g: 3.151\n",
            "[143/300]: loss_d: 0.234, loss_g: 3.217\n",
            "[144/300]: loss_d: 0.237, loss_g: 3.176\n",
            "[145/300]: loss_d: 0.230, loss_g: 3.209\n",
            "[146/300]: loss_d: 0.228, loss_g: 3.268\n",
            "[147/300]: loss_d: 0.232, loss_g: 3.240\n",
            "[148/300]: loss_d: 0.230, loss_g: 3.225\n",
            "[149/300]: loss_d: 0.229, loss_g: 3.251\n",
            "[150/300]: loss_d: 0.233, loss_g: 3.210\n",
            "[151/300]: loss_d: 0.225, loss_g: 3.229\n",
            "[152/300]: loss_d: 0.230, loss_g: 3.246\n",
            "[153/300]: loss_d: 0.228, loss_g: 3.247\n",
            "[154/300]: loss_d: 0.224, loss_g: 3.247\n",
            "[155/300]: loss_d: 0.222, loss_g: 3.253\n",
            "[156/300]: loss_d: 0.229, loss_g: 3.264\n",
            "[157/300]: loss_d: 0.231, loss_g: 3.219\n",
            "[158/300]: loss_d: 0.223, loss_g: 3.287\n",
            "[159/300]: loss_d: 0.229, loss_g: 3.240\n",
            "[160/300]: loss_d: 0.226, loss_g: 3.235\n",
            "[161/300]: loss_d: 0.222, loss_g: 3.276\n",
            "[162/300]: loss_d: 0.223, loss_g: 3.256\n",
            "[163/300]: loss_d: 0.218, loss_g: 3.317\n",
            "[164/300]: loss_d: 0.221, loss_g: 3.269\n",
            "[165/300]: loss_d: 0.220, loss_g: 3.280\n",
            "[166/300]: loss_d: 0.218, loss_g: 3.320\n",
            "[167/300]: loss_d: 0.217, loss_g: 3.313\n",
            "[168/300]: loss_d: 0.222, loss_g: 3.335\n",
            "[169/300]: loss_d: 0.224, loss_g: 3.266\n",
            "[170/300]: loss_d: 0.217, loss_g: 3.307\n",
            "[171/300]: loss_d: 0.222, loss_g: 3.310\n",
            "[172/300]: loss_d: 0.212, loss_g: 3.290\n",
            "[173/300]: loss_d: 0.217, loss_g: 3.324\n",
            "[174/300]: loss_d: 0.218, loss_g: 3.315\n",
            "[175/300]: loss_d: 0.211, loss_g: 3.337\n",
            "[176/300]: loss_d: 0.220, loss_g: 3.330\n",
            "[177/300]: loss_d: 0.209, loss_g: 3.290\n",
            "[178/300]: loss_d: 0.208, loss_g: 3.367\n",
            "[179/300]: loss_d: 0.209, loss_g: 3.388\n",
            "[180/300]: loss_d: 0.206, loss_g: 3.379\n",
            "[181/300]: loss_d: 0.209, loss_g: 3.388\n",
            "[182/300]: loss_d: 0.204, loss_g: 3.392\n",
            "[183/300]: loss_d: 0.207, loss_g: 3.407\n",
            "[184/300]: loss_d: 0.208, loss_g: 3.391\n",
            "[185/300]: loss_d: 0.212, loss_g: 3.360\n",
            "[186/300]: loss_d: 0.210, loss_g: 3.377\n",
            "[187/300]: loss_d: 0.199, loss_g: 3.406\n",
            "[188/300]: loss_d: 0.200, loss_g: 3.415\n",
            "[189/300]: loss_d: 0.198, loss_g: 3.489\n",
            "[190/300]: loss_d: 0.206, loss_g: 3.392\n",
            "[191/300]: loss_d: 0.205, loss_g: 3.393\n",
            "[192/300]: loss_d: 0.200, loss_g: 3.404\n",
            "[193/300]: loss_d: 0.198, loss_g: 3.430\n",
            "[194/300]: loss_d: 0.199, loss_g: 3.426\n",
            "[195/300]: loss_d: 0.197, loss_g: 3.457\n",
            "[196/300]: loss_d: 0.190, loss_g: 3.467\n",
            "[197/300]: loss_d: 0.196, loss_g: 3.462\n",
            "[198/300]: loss_d: 0.191, loss_g: 3.498\n",
            "[199/300]: loss_d: 0.185, loss_g: 3.563\n",
            "[200/300]: loss_d: 0.196, loss_g: 3.504\n",
            "[201/300]: loss_d: 0.191, loss_g: 3.445\n",
            "[202/300]: loss_d: 0.187, loss_g: 3.547\n",
            "[203/300]: loss_d: 0.196, loss_g: 3.519\n",
            "[204/300]: loss_d: 0.200, loss_g: 3.478\n",
            "[205/300]: loss_d: 0.190, loss_g: 3.500\n",
            "[206/300]: loss_d: 0.197, loss_g: 3.494\n",
            "[207/300]: loss_d: 0.190, loss_g: 3.495\n",
            "[208/300]: loss_d: 0.188, loss_g: 3.485\n",
            "[209/300]: loss_d: 0.191, loss_g: 3.512\n",
            "[210/300]: loss_d: 0.195, loss_g: 3.515\n",
            "[211/300]: loss_d: 0.191, loss_g: 3.509\n",
            "[212/300]: loss_d: 0.188, loss_g: 3.515\n",
            "[213/300]: loss_d: 0.188, loss_g: 3.542\n",
            "[214/300]: loss_d: 0.194, loss_g: 3.509\n",
            "[215/300]: loss_d: 0.192, loss_g: 3.509\n",
            "[216/300]: loss_d: 0.192, loss_g: 3.513\n",
            "[217/300]: loss_d: 0.185, loss_g: 3.543\n",
            "[218/300]: loss_d: 0.187, loss_g: 3.522\n",
            "[219/300]: loss_d: 0.188, loss_g: 3.501\n",
            "[220/300]: loss_d: 0.188, loss_g: 3.523\n",
            "[221/300]: loss_d: 0.183, loss_g: 3.572\n",
            "[222/300]: loss_d: 0.185, loss_g: 3.569\n",
            "[223/300]: loss_d: 0.183, loss_g: 3.543\n",
            "[224/300]: loss_d: 0.186, loss_g: 3.555\n",
            "[225/300]: loss_d: 0.174, loss_g: 3.630\n",
            "[226/300]: loss_d: 0.183, loss_g: 3.600\n",
            "[227/300]: loss_d: 0.178, loss_g: 3.652\n",
            "[228/300]: loss_d: 0.178, loss_g: 3.607\n",
            "[229/300]: loss_d: 0.182, loss_g: 3.624\n",
            "[230/300]: loss_d: 0.177, loss_g: 3.563\n",
            "[231/300]: loss_d: 0.185, loss_g: 3.600\n",
            "[232/300]: loss_d: 0.180, loss_g: 3.609\n",
            "[233/300]: loss_d: 0.179, loss_g: 3.606\n",
            "[234/300]: loss_d: 0.183, loss_g: 3.576\n",
            "[235/300]: loss_d: 0.178, loss_g: 3.651\n",
            "[236/300]: loss_d: 0.175, loss_g: 3.623\n",
            "[237/300]: loss_d: 0.175, loss_g: 3.649\n",
            "[238/300]: loss_d: 0.167, loss_g: 3.724\n",
            "[239/300]: loss_d: 0.168, loss_g: 3.734\n",
            "[240/300]: loss_d: 0.180, loss_g: 3.669\n",
            "[241/300]: loss_d: 0.181, loss_g: 3.633\n",
            "[242/300]: loss_d: 0.187, loss_g: 3.591\n",
            "[243/300]: loss_d: 0.183, loss_g: 3.616\n",
            "[244/300]: loss_d: 0.176, loss_g: 3.646\n",
            "[245/300]: loss_d: 0.171, loss_g: 3.661\n",
            "[246/300]: loss_d: 0.172, loss_g: 3.754\n",
            "[247/300]: loss_d: 0.164, loss_g: 3.653\n",
            "[248/300]: loss_d: 0.176, loss_g: 3.723\n",
            "[249/300]: loss_d: 0.168, loss_g: 3.698\n",
            "[250/300]: loss_d: 0.189, loss_g: 3.619\n",
            "[251/300]: loss_d: 0.183, loss_g: 3.612\n",
            "[252/300]: loss_d: 0.182, loss_g: 3.588\n",
            "[253/300]: loss_d: 0.170, loss_g: 3.719\n",
            "[254/300]: loss_d: 0.167, loss_g: 3.752\n",
            "[255/300]: loss_d: 0.156, loss_g: 3.769\n",
            "[256/300]: loss_d: 0.173, loss_g: 3.683\n",
            "[257/300]: loss_d: 0.173, loss_g: 3.707\n",
            "[258/300]: loss_d: 0.169, loss_g: 3.670\n",
            "[259/300]: loss_d: 0.161, loss_g: 3.738\n",
            "[260/300]: loss_d: 0.175, loss_g: 3.690\n",
            "[261/300]: loss_d: 0.162, loss_g: 3.691\n",
            "[262/300]: loss_d: 0.167, loss_g: 3.736\n",
            "[263/300]: loss_d: 0.168, loss_g: 3.734\n",
            "[264/300]: loss_d: 0.148, loss_g: 3.872\n",
            "[265/300]: loss_d: 0.165, loss_g: 3.769\n",
            "[266/300]: loss_d: 0.152, loss_g: 3.859\n",
            "[267/300]: loss_d: 0.168, loss_g: 3.797\n",
            "[268/300]: loss_d: 0.160, loss_g: 3.713\n",
            "[269/300]: loss_d: 0.152, loss_g: 3.830\n",
            "[270/300]: loss_d: 0.162, loss_g: 3.824\n",
            "[271/300]: loss_d: 0.175, loss_g: 3.733\n",
            "[272/300]: loss_d: 0.159, loss_g: 3.750\n",
            "[273/300]: loss_d: 0.160, loss_g: 3.780\n",
            "[274/300]: loss_d: 0.162, loss_g: 3.796\n",
            "[275/300]: loss_d: 0.157, loss_g: 3.820\n",
            "[276/300]: loss_d: 0.163, loss_g: 3.739\n",
            "[277/300]: loss_d: 0.162, loss_g: 3.772\n",
            "[278/300]: loss_d: 0.161, loss_g: 3.781\n",
            "[279/300]: loss_d: 0.154, loss_g: 3.841\n",
            "[280/300]: loss_d: 0.152, loss_g: 3.895\n",
            "[281/300]: loss_d: 0.160, loss_g: 3.934\n",
            "[282/300]: loss_d: 0.167, loss_g: 3.760\n",
            "[283/300]: loss_d: 0.155, loss_g: 3.828\n",
            "[284/300]: loss_d: 0.142, loss_g: 3.965\n",
            "[285/300]: loss_d: 0.167, loss_g: 3.818\n",
            "[286/300]: loss_d: 0.156, loss_g: 3.840\n",
            "[287/300]: loss_d: 0.154, loss_g: 3.893\n",
            "[288/300]: loss_d: 0.158, loss_g: 3.825\n",
            "[289/300]: loss_d: 0.161, loss_g: 3.772\n",
            "[290/300]: loss_d: 0.160, loss_g: 3.830\n",
            "[291/300]: loss_d: 0.151, loss_g: 3.907\n",
            "[292/300]: loss_d: 0.157, loss_g: 3.848\n",
            "[293/300]: loss_d: 0.157, loss_g: 3.821\n",
            "[294/300]: loss_d: 0.153, loss_g: 3.824\n",
            "[295/300]: loss_d: 0.149, loss_g: 3.897\n",
            "[296/300]: loss_d: 0.166, loss_g: 3.779\n",
            "[297/300]: loss_d: 0.153, loss_g: 3.857\n",
            "[298/300]: loss_d: 0.148, loss_g: 3.888\n",
            "[299/300]: loss_d: 0.153, loss_g: 3.926\n",
            "[300/300]: loss_d: 0.155, loss_g: 3.913\n"
          ]
        }
      ],
      "source": [
        "n_epoch = 300\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    D_losses, G_losses = [], []\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        D_losses.append(D_train(x))\n",
        "        G_losses.append(G_train(x))\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))\n",
        "\n",
        "    generate_test_image(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "f4mDu4VE2PAz"
      },
      "outputs": [],
      "source": [
        "generated = G(test_z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "MtcG-dAp2PA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba6c39c-68cd-4645-a289-99f9de3d3c83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "generated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "jjOeB0h92PA0"
      },
      "outputs": [],
      "source": [
        "save_image(generated.view(generated.size(0), 1, 28, 28),\n",
        "                   f'./output/all.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}